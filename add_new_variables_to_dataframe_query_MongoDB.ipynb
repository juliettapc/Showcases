{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a set of new variables for the analysis of in-text-citations\n",
    "\n",
    "In this code I take a file with some basic information about citation patterns among academic papers, and I add some new, more sophisticated variables, derived from the basic ones. Some of these new variables are calculated directly by combining the basic ones, while in other cases, I query a Mongo DataBase to obtain extra information that I need. Once I'm done, I dump the new dataframe in a pickle file.\n",
    "\n",
    "See notebook \"For_UPTAKE_get_basic_stats_plots_and_null_model\", where I actually use some of these variables for plotting and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import gzip\n",
    "import os,glob\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "import random\n",
    "from  scipy import stats\n",
    "import datetime\n",
    "import math\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "\n",
    "from IPython.core.display import display,HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))  # to make the notebook use the entire width of the browser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload main datafiles\n",
    "\n",
    "These are the basic files with information about citations among academic papers that I will use to built upon. These files are pickled pandas dataframes, and were created elsewhere.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "%time df_merged = pickle.load(open('../data/df_reference_cite_plos_merged_with_team_expertise_and_recycled_ref.pkl', 'rb'))\n",
    "print (\"done loading pickle\", df_merged.shape)\n",
    "\n",
    "\n",
    "\n",
    "%time plos_df = pickle.load(open('../data/plos_paper_dataframe_ONLY_ARTICLES_num_ref_sect_young_old.pkl', 'rb'))\n",
    "print (\"done loading pickle\", plos_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I modify the unique ID of the papers to make it standard across entries, and add it as new column of the main dataframe, dropping the old one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fix_paper_UT(row):\n",
    "    paper_UT = row.paper_UT\n",
    "    return '000'+str(paper_UT)\n",
    "\n",
    "##################3\n",
    "\n",
    "df_merged['new_paper_UT'] = df_merged.apply (lambda row: fix_paper_UT(row),axis=1)\n",
    "df_merged.drop(['paper_UT'], axis=1, inplace=True)\n",
    "df_merged.rename(columns={'new_paper_UT': 'paper_UT'}, inplace=True)\n",
    "\n",
    "df_merged[['paper_UT']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I create a number of simple, derived variables and add them to the datafram as new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_log_num_cit_of_paper(row):\n",
    "    \"\"\"\n",
    "    This function takes one row from the dataframe and returns the corresponding value for the new variable (logarithm of the \n",
    "    number of citations of the paper) to be added in a column\n",
    "    \"\"\"\n",
    "  \n",
    "    try:\n",
    "        log_num_cit=np.log10(float(row.paper_cite_count))\n",
    "    except:\n",
    "        print (row.plos_pub_year)\n",
    "        log_num_cit=0.\n",
    "    return log_num_cit\n",
    "\n",
    "###################\n",
    "\n",
    "\n",
    "def get_diff_publ_plos_publ_ref(row):\n",
    "    \"\"\"\n",
    "    This function takes one row from the dataframe and returns the corresponding value for the new variable (age difference \n",
    "    between the reference and the citing paper) to be added in a column\n",
    "    \"\"\"\n",
    "  \n",
    "    try:\n",
    "        delta_publ_year=float(row.plos_pub_year)- float(row.ref_pub_year) \n",
    "    except:\n",
    "        delta_publ_year=np.nan\n",
    "    return delta_publ_year\n",
    "\n",
    "###################\n",
    "\n",
    "def get_relative_position_within_section(row):\n",
    "    \"\"\"\n",
    "    This function takes one row from the dataframe and returns the corresponding value for the new variable (relative position\n",
    "    of the reference in the paper section, in units of characters) to be added in a column\n",
    "    \"\"\"\n",
    "  \n",
    "    try:\n",
    "        rel_pos=float(row.sect_char_pos) / float(row.sect_char_total) \n",
    "    except:\n",
    "        rel_pos=np.nan\n",
    "    return rel_pos \n",
    "\n",
    "###################\n",
    "\n",
    "def get_section(row):\n",
    "    \"\"\"\n",
    "    This function takes one row from the dataframe and returns the corresponding value for the new variable (numberical code \n",
    "    to identify the section of the paper where the reference is used) to be added in a column\n",
    "    \"\"\"\n",
    "  \n",
    "    sect_num = row.regex_sect_index\n",
    "    section_name = \"NA\"\n",
    "    \n",
    "    if sect_num == 0:\n",
    "        section_name = \"0:Intro\"\n",
    "    elif sect_num == 1:\n",
    "        section_name = \"1:Methods\"\n",
    "    elif sect_num == 2:\n",
    "        section_name = \"2:Results\"\n",
    "    elif sect_num == 3:\n",
    "        section_name = \"3:Discussion\"\n",
    "    elif sect_num == 4:\n",
    "        section_name = \"4:Results/Discussion\"\n",
    "    elif sect_num == 5:\n",
    "        section_name = \"5:Conclusion\"\n",
    "    elif sect_num == 6:\n",
    "        section_name = \"6:Mx\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    return section_name \n",
    "\n",
    "###################\n",
    "\n",
    "\n",
    "\n",
    "%time df_merged['log10_num_cit_ref'] = df_merged.apply (lambda row: get_log_num_cit_of_ref(row),axis=1)\n",
    "%time df_merged['log10_num_cit_paper'] = df_merged.apply (lambda row: get_log_num_cit_of_paper(row),axis=1)\n",
    "%time df_merged['diff_year_plos_ref'] = df_merged.apply (lambda row: get_diff_publ_plos_publ_ref(row),axis=1)\n",
    "%time df_merged['rel_loc_in_sect'] = df_merged.apply (lambda row: get_relative_position_within_section(row),axis=1)\n",
    "%time df_merged['section'] = df_merged.apply (lambda row: get_section(row),axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  In this next function, I assign a label to each usage of a reference (that is, to each pair reference-paper), to mark it as 'isolated' or 'group' reference, depending on how it is used in a given paper in my records. I'll use this later in the analysis to select subsets of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolated_or_group_citation(row):\n",
    "    \"\"\"\n",
    "    This function takes one row from the dataframe and returns a flag (1 or 0) to mark the usage of isolated or group references,\n",
    "    to be added in a column\n",
    "    \"\"\"\n",
    "  \n",
    "    window=4  # max distance (in units of characters) to determine if a reference is clustered with anothers or not\n",
    "    \n",
    "    paper_UT = row.paper_UT\n",
    "    reference_UT = row.reference_UT\n",
    "    regex_sect_index = row.regex_sect_index\n",
    "    sect_char_pos = row.sect_char_pos\n",
    "    \n",
    "    \n",
    "    df_selection=df_merged[ (df_merged['paper_UT'] == paper_UT) &  (df_merged['regex_sect_index'] == regex_sect_index) ]   \n",
    "    \n",
    "   \n",
    "    list_positions=sorted(list(df_selection.sect_char_pos))\n",
    "    \n",
    "   \n",
    "    lista_diff_pos= sorted([abs(item - sect_char_pos ) for item in list_positions])[1:]\n",
    "    \n",
    "     \n",
    "    if len(lista_diff_pos)>0:\n",
    "        if min(lista_diff_pos) > window:\n",
    "            \n",
    "            return 1   # isolated reference\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "\n",
    "###################\n",
    "\n",
    "\n",
    "%time df_merged['isolated_citation'] = df_merged.apply (lambda row: isolated_or_group_citation(row),axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After adding a number of new columns, I wrote the dataframe to a pickle file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print (\"writing pickle.....\")\n",
    "\n",
    "\n",
    "path = '../data/df_reference_cite_plos_merged_simplified_added_more_columns.pkl'\n",
    "%time df_merged.to_pickle(path, compression='infer', protocol=2)\n",
    "\n",
    "print (\"written:\",path)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, I label each record of references used in a given paper as self-citations or not self-citations\n",
    "\n",
    "In order to do this, I need to check who are the authors of a given paper, as well as who are the authors of all the references this paper cites, and find any possible matches.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, I need a list of the unique IDs of both the PLoS papers and all references that appear in my citation data. These lists are created elsewhere, and uploaded here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_all_plos_UTs = pickle.load(open('../data/lista_all_plos_UTs.pkl', 'rb'))\n",
    "new_lista_all_plos_UTs = ['000'+str(item) for item in lista_all_plos_UTs]   # i need to add the 000 so it matches the db UTs\n",
    "lista_all_reference_UTs = pickle.load(open('../data/lista_all_reference_UTs.pkl', 'rb'))\n",
    "lista_UTs_plos_and_ref = new_lista_all_plos_UTs+ lista_all_reference_UTs\n",
    "lista_UTs_plos_and_ref =  list(set(lista_UTs_plos_and_ref))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the author IDs, I use an author-disambiguated dataset (that I compiled elsewhere, and that it is based on proprietary Web of Science disambiguated data). Web of Science assigns unique IDs to papers (called UT), as well as unique IDs to authors (called DAIS).\n",
    "The author-disambiguated data is structured as follows: one row per paper, and in each row: UT, list_if_DAIS. \n",
    "\n",
    "The data is storaged in multiple (320) zip files, and it includes many more records than the ones I need: my dataset is 156K PLoS papers, while the entire Web of Science includes around 60M papers. Hence, as I read the zip files, I only keep the info about the papers I care about. I will concatenate the files, and create a new pandas dataframe with the author-disambiguated info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###I create a list with all the names of the zip files to iterate over\n",
    "\n",
    "path_disamb='/home/workspace/scibio/resources/rbusa/rbusa_main_v1_1/disambiguation/wos_dais/'\n",
    "\n",
    "lista_files=[]\n",
    "\n",
    "for i in range(320):\n",
    "    i += 1\n",
    "    file_name=path_disamb+'wos_dais_all_batch'+str(i)+'.csv.gz'    \n",
    "    lista_files.append(file_name)\n",
    "##############\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cont_tot=0\n",
    "cont_filtered=0\n",
    "########### concatenate a list of (gzip) files into one single pandas dataframe:\n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "for file_ in lista_files:\n",
    "    print (file_)\n",
    "    df = pd.read_csv(file_, compression = 'gzip', dtype=object, index_col=None, header=0)   \n",
    "    cont_tot += len(df)\n",
    "    df = df[df['WOS'].isin(lista_UTs_plos_and_ref)]  # i directly filter out what i dont need, to save space\n",
    "    cont_filtered += len(df)\n",
    "    list_.append(df)\n",
    "\n",
    "df_disamb = pd.concat(list_)\n",
    "print (cont_tot, cont_filtered)    # 208,042,832         14,382,344\n",
    "\n",
    "\n",
    "df_disamb.rename(columns={'WOS': 'UT'}, inplace=True)\n",
    "print (df_disamb.shape)     # (14382344, 3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path = '../data/df_disambig_filtered.pkl'\n",
    "%time df_disamb.to_pickle(path, compression='infer', protocol=2)\n",
    "\n",
    "print (\"written:\",path)  \n",
    "df_disamb.head()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, from the dataframe I have just created, I build a dictionary that gives me the list of disambiguated authors for a given paper ID. I'll need this for the function that labels citations as self or not self citations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dict_UT_list_authors={}    \n",
    "\n",
    "for item in df_disamb.groupby(['UT']):        \n",
    "\n",
    "    UT=item[0]\n",
    "    dict_UT_list_authors[UT]=list(item[1].DAIS)\n",
    "\n",
    "\n",
    "print (\"done. now writing pickle.....\")\n",
    "\n",
    "with open('../data/dict_UT_list_authors.pkl', 'wb') as handle:\n",
    "     pickle.dump(dict_UT_list_authors, handle, protocol = 2)\n",
    "print (\"written:\",'../data/dict_UT_list_authors.pkl')   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I apply a function to each row of my dataframe (each row corresponds to the occurrence of one reference in one particular paper), \n",
    "to figure out whether a citation is self-citation or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_self_citation(row, dict_UT_list_authors, cont_missing_records):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function labels a given record of paper-citation in the dataframe as self-citation or not self-citation\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : a row from a dataframe\n",
    "    \n",
    "    dict_UT_list_authors : dictionary where the key is the UT (ID) of papers\n",
    "                            and the value is the list of disambiguated authors in that paper\n",
    "                            \n",
    "    cont_missing_records : int\n",
    "        counter for the number of paper UT not found\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        a flag to identify self-citation (1) or not self-citation (0)\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "      \n",
    "    \n",
    "    paper_UT = row.paper_UT\n",
    "    ref_UT = row.reference_UT\n",
    "    \n",
    "    self_citation=0\n",
    "    try:\n",
    "        lista_DAIS_paper = dict_UT_list_authors[paper_UT]\n",
    "    except KeyError:\n",
    "        lista_DAIS_paper = []\n",
    "        cont_missing_records +=1\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        lista_DAIS_ref = dict_UT_list_authors[ref_UT]\n",
    "    except KeyError:\n",
    "        lista_DAIS_ref = []\n",
    "        cont_missing_records +=1\n",
    "        \n",
    "   \n",
    "    if len(set(lista_DAIS_paper) & set(lista_DAIS_ref)) >0:  # if the citing paper and the reference paper share at least one author\n",
    "        self_citation = 1\n",
    "       \n",
    "        \n",
    "    \n",
    "    return self_citation\n",
    "\n",
    "###################\n",
    "\n",
    "cont_missing_records=0\n",
    "%time df_merged['self_citation'] = df_merged.apply (lambda row: get_self_citation(row, dict_UT_list_authors, cont_missing_records), axis=1)\n",
    "\n",
    "print (\"done. missing records (plos and/or ref):\", cont_missing_records, \"\\nwriting pickle.....\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, I want to extract the number of early citations of young references. Because this info is not among the basic varibles in the original dataframe, I'll have to query the Mongo database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Establish MongoDB connection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MongoConnection(object):\n",
    "    def __init__(self, cxnSettings, **kwargs):\n",
    "        self.settings = cxnSettings\n",
    "        self.mongoURI = self._constructURI()\n",
    "        self.connect(**kwargs)\n",
    "        self.ensure_index()\n",
    "\n",
    "    def _constructURI(self):\n",
    "        '''\n",
    "        Construct the mongo URI\n",
    "        '''\n",
    "        mongoURI = 'mongodb://'\n",
    "        #User/password handling\n",
    "        if 'user'in self.settings and 'password' in self.settings:\n",
    "            mongoURI += self.settings['user'] + ':' + self.settings['password']\n",
    "            mongoURI += '@'\n",
    "        elif 'user' in self.settings:\n",
    "            print('Missing password for given user, proceeding without either')\n",
    "        elif 'password' in self.settings:\n",
    "            print('Missing user for given passord, proceeding without either')\n",
    "        #Host and port\n",
    "        try:\n",
    "            mongoURI += self.settings['host'] + ':'\n",
    "        except KeyError:\n",
    "            print('Missing the hostname. Cannot connect without host')\n",
    "            sys.exit()\n",
    "        try:\n",
    "            mongoURI += str(self.settings['port'])\n",
    "        except KeyError:\n",
    "            print('Missing the port. Substituting default port of 27017')\n",
    "            mongoURI += str('27017')\n",
    "        return mongoURI\n",
    "\n",
    "    def connect(self, **kwargs):\n",
    "        '''\n",
    "        Establish the connection, database, and collection\n",
    "        '''\n",
    "        self.connection = MongoClient(self.mongoURI, **kwargs)\n",
    "        #########\n",
    "        try:\n",
    "            self.db = self.connection[self.settings['db']]\n",
    "        except KeyError:\n",
    "            print(\"Must specify a database as a 'db' key in the settings file\")\n",
    "            sys.exit()\n",
    "        #########\n",
    "        try:\n",
    "            self.collection = self.db[self.settings['collection']]\n",
    "        except KeyError:\n",
    "            print('Should have a collection.', end='')\n",
    "            print('Starting a collection in database', end='')\n",
    "            print(' for current connection as test.')\n",
    "            self.collection = self.db['test']\n",
    "\n",
    "    def tearDown(self):\n",
    "        '''\n",
    "        Closes the connection\n",
    "        '''\n",
    "        self.connection.close()\n",
    "\n",
    "    def ensure_index(self):\n",
    "        '''\n",
    "        Ensures the connection has all given indexes.\n",
    "        indexes: list of (`key`, `direction`) pairs.\n",
    "            See docs.mongodb.org/manual/core/indexes/ for possible `direction`\n",
    "            values.\n",
    "        '''\n",
    "        if 'indexes' in self.settings:\n",
    "            for index in self.settings['indexes']:\n",
    "                self.collection.ensure_index(index[0], **index[1])\n",
    "\n",
    "\n",
    "\n",
    "#############################  (not the real user name nor password)\n",
    "\n",
    "merged_papers_settings = {\n",
    "    \"host\": \"chicago.chem-eng.northwestern.edu\",\n",
    "    \"port\": \"27017\",\n",
    "    \"db\": \"web_of_science_aux\",\n",
    "    \"collection\": \"merged_papers\",\n",
    "    \"user\": \"lalalalala\",  \n",
    "    \"password\": \"lalalalalalylala\"\n",
    "}\n",
    "\n",
    "papers_con = MongoConnection(merged_papers_settings)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the number of early citations of a reference, I proceed as follows: First, I pick a focus year, and select all the PLoS papper published that year, as well as all the references that those papers cite. Then, I filter the references to select only the ones that were young at the time of the publication of the citing paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_plos_year = 2009  # available years: 2005 to 2016\n",
    "def_young = 1  # young references are <= 1 year old\n",
    "   \n",
    "\n",
    "df_selection_general_young_ref = df_merged[ (df_merged['ref_pub_year'] >= (focus_plos_year - def_young) )]  \n",
    "\n",
    "list_UT_young_ref_by_focus_year = df_selection_general_young_ref.reference_UT.unique()\n",
    "\n",
    "\n",
    "print (\"\\n\\nTotal # records of young ref. by\", focus_plos_year, \": \",df_selection_general_young_ref.shape, \"  # unique ref_UTs:\", len(list_UT_young_ref_by_focus_year))   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in order to plot the results later, all references need to have a value of early citations assigned (even for those that do not qualify as young references). So, first, I initialize all values to Nan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dict_UT_young_ref_num_cit_by_focus_year = {}\n",
    "\n",
    "for ref_UT in list_all_ref_UT:  # i need to assign a value to all references  (those that were older, get a NaN, those that are young enough (one year old or younger), get whatever number of citations they had by focus_year)           \n",
    "    dict_UT_young_ref_num_cit_by_focus_year[ref_UT] = np.nan  # first i initialize all ref to nan  (faster than evaluating whether the reference is in the list of selected ones or not)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I iterate over all young references in the focus year, and I query the database to obtain the number of citaions those papers received in or before the focus year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "cont = 0    \n",
    "for ref_UT in list_UT_young_ref_by_focus_year:  \n",
    "\n",
    "    ref_UT = str(ref_UT)  # i make sure it is a string\n",
    "  \n",
    "    item = papers_con.collection.find_one({\"UT\" : ref_UT}, {'citations':1})  # the output of a find_one cursor is a DICTIONARY!!!   citations:   cites received by the paper (currently updated value!)\n",
    "\n",
    "\n",
    "    dict_UT_young_ref_num_cit_by_focus_year[ref_UT] = 0\n",
    "\n",
    "    try:\n",
    "        list_citing_papers = item[\"citations\"]                                \n",
    "\n",
    "        aux_list_citing_papers = list_citing_papers\n",
    "\n",
    "\n",
    "\n",
    "        ### QUERY TO GET ONLY CITATIONS RECEIVED BY THE REFERENCE PAPER IN THE FOCUS_YEAR OR EARLIER:\n",
    "        cursor2_count = papers_con.collection.find( {  \"UT\" :{\"$in\":aux_list_citing_papers }, \"issue.PY\": { '$lte': focus_plos_year }} ).count()  \n",
    "       \n",
    "\n",
    "        #### i save the value for number of early citations in a dictionary\n",
    "        dict_UT_young_ref_num_cit_by_focus_year[ref_UT] = cursor2_count\n",
    "\n",
    "\n",
    "\n",
    "    except :  # IF THERE ARE NO CITATIONS IN WEB OF SCIENCE FOR THAT REFERENCE PAPER   \n",
    "         dict_UT_young_ref_num_cit_by_focus_year[ref_UT] = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(cont)#, ref_UT, \"# tot cit:\",count_cursor1, \"# cit by\",focus_year, \":\",dict_UT_young_ref_num_cit_by_focus_year[ref_UT])       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    cont +=1  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I dump the dictionary into a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print (\"done.\", len(dict_UT_young_ref_num_cit_by_focus_year),\"dumping dict ref.......\")         \n",
    "with open('../data/dict_UT_young_ref_in'+str(focus_plos_year)+'_num_cit_by_'+str(focus_plos_year)+'.pkl', 'wb') as handle:\n",
    "         pickle.dump(dict_UT_young_ref_num_cit_by_focus_year, handle, protocol = 2)\n",
    "print (\"written:\",'../data/dict_UT_young_ref_in'+str(focus_plos_year)+'_num_cit_by_'+str(focus_plos_year)+'.pkl')   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I add a new column to my dataframe with the new info about early citations accrued by young references (for a given focus year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_field(row, dict_UT_young_ref_in_focus_year_num_cit):\n",
    "    \"\"\"\n",
    "    This function finds the number of early citations of a reference in the provided dictionary and returns the value, \n",
    "    to be added as new column in the existing dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row : a row from a dataframe\n",
    "    \n",
    "    dict_UT_young_ref_in_focus_year_num_cit : dictionary where the key is the UT (ID) of the reference\n",
    "                            and the value is the number of early citations the reference has accrued by the focus year\n",
    "                            \n",
    "    \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        number of early citations of the reference\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ref_UT=str(row.reference_UT) \n",
    "    \n",
    "    num_cit_young_ref = dict_UT_young_ref_in_focus_year_num_cit[ref_UT]  \n",
    "    \n",
    "    return num_cit_young_ref\n",
    "    \n",
    "    ###################\n",
    "    \n",
    "    \n",
    "    \n",
    "focus_year = 2009\n",
    "dict_UT_young_ref_in_focus_year_num_cit = pickle.load(open('../data/dict_UT_young_ref_in'+str(focus_year)+'_num_cit_by_'+str(focus_year)+'.pkl', 'rb'))\n",
    "\n",
    "column_name = 'num_cit_young_ref_by'+str(focus_year)\n",
    "%time df_merged[column_name] = df_merged.apply (lambda row: get_new_field(row, dict_UT_young_ref_in_focus_year_num_cit),axis=1)\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I dump the dataframe with the new column to a pickle file one more time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print (\"writing pickle.....\")\n",
    "\n",
    "\n",
    "path = '../data/df_reference_cite_plos_merged_simplified_added_more_columns.pkl'\n",
    "%time df_merged.to_pickle(path, compression='infer', protocol=2)\n",
    "\n",
    "print (\"written:\",path)  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
