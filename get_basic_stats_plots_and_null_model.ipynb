{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of in-text-citations\n",
    "\n",
    "In this project I explore the different citation patterns of successful and unssesful papers. I use a dataset that includes information on 156K papers from the PLoS journals (Public Library of Science) between 2005 and 2016, citing 2.5M unique papers, and making up to 6.8M records in total (each record is a pair paper-reference).\n",
    "\n",
    "First, I read the datafile and I do some exploratory statistical analysis. \n",
    "\n",
    "Next, I plot several figures, for example, the age of the references used by a paper, as a function of the paper section as well as the future impact (number of citations) of the paper itself. I control for publication year and subject field. Similarly, I obtain the corresponding plot for the number of citations of the references used by PLoS papers.\n",
    "\n",
    "Finally, I run a null-model to compare the actual usage of top and botoom references by top and bottom PLoS papers ('top' and 'bottom' defined by the percentile of number of citations received in its cohort year), to the expected values. The randomization process preserves certain basic properties of the data, such as publication year, subject field, and groups of references cited together in the same cluster in a given paper. I then plot the actual vs expected values, as well as computing the corresponding z-scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import copy\n",
    "import pickle\n",
    "import gzip\n",
    "import os,glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "import random\n",
    "from  scipy import stats\n",
    "import math\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "import plotly.figure_factory as ff\n",
    "import plotly\n",
    "plotly.tools.set_credentials_file(username='juliettapc', api_key='nM6iUdx6dGaOiPXQTwpP')   # go to: https://plot.ly/settings/api#/   for a new key if needed\n",
    "\n",
    "\n",
    "########## to be able to plot offline (without sending the plots to the plotly server every time)\n",
    "import plotly.offline as offline\n",
    "from plotly.graph_objs import *\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "################\n",
    "\n",
    "\n",
    "\n",
    "####### to make the notebook use the entire width of the browser\n",
    "from IPython.core.display import display,HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########  upload the main datafile        \n",
    "\n",
    "%time df_merged = pickle.load(open('../data/df_reference_cite_plos_merged_simplified_added_more_columns_no_self-cit_one_ref_per_sect_ONLY_ARTICLES.pkl', 'rb'))\n",
    "print (\"done loading pickles\", df_merged.shape)\n",
    "\n",
    "\n",
    "\n",
    "######  upload auxiliary file with info only on plos paper \n",
    "\n",
    "%time plos_df = pickle.load(open('../data/plos_paper_dataframe_ONLY_ARTICLES_num_ref_sect_young_old.pkl', 'rb'))\n",
    "print (\"done loading plos_df\", plos_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######## i load the dictionary for category-code\n",
    "\n",
    "dict_categ_code = pickle.load(open('../data/dict_categ_code.pkl', 'rb'))\n",
    "\n",
    "\n",
    "print (\"from the entire df \",df_merged.shape, \"; and plos one records:\",  df_merged[df_merged['plos_j1']== \"PLOS ONE\"].shape,\"\\n\\n\")\n",
    "# {'Biology and life sciences': 0,\n",
    "#  'Computer and information sciences': 1,\n",
    "#  'Earth sciences': 2,\n",
    "#  'Ecology and environmental sciences': 3,\n",
    "#  'Engineering and technology': 4,\n",
    "#  'Medicine and health sciences': 5,\n",
    "#  'People and places': 6,\n",
    "#  'Physical sciences': 7,\n",
    "#  'Research and analysis methods': 8,\n",
    "#  'Science policy': 9,\n",
    "#  'Social sciences': 10}\n",
    "\n",
    "dict_code_categ={}\n",
    "dict_size_categ={}\n",
    "for categ in dict_categ_code:\n",
    "    code = str(dict_categ_code[categ])\n",
    "    \n",
    "    df_selection_categ = df_merged[df_merged['categ_codes'].str.contains(code)]\n",
    "   # print (categ, code, df_selection_categ.shape)\n",
    "    size= len(df_selection_categ)\n",
    "    dict_size_categ[size] = categ\n",
    "    dict_code_categ[code] = categ\n",
    "\n",
    "for size in reversed(sorted(dict_size_categ)):\n",
    "    print (size, dict_size_categ[size])\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some basic data exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### basic data exploration:\n",
    "##########################################\n",
    "\n",
    "#ALL sections:  {'intro': 0, 'methods': 1, 'results': 2, 'disc': 3, 'res_disc':4, 'concl':5, 'mixed':6, 'na':7}\n",
    "\n",
    "# df_merged.regex_sect_index.value_counts()\n",
    "# 0    2188159 / 5787630. =  0.37807\n",
    "# 3    2105019 / 5787630. =  0.36371\n",
    "# 1     678770 / 5787630. =  0.11728\n",
    "# 2     563650 / 5787630. =  0.09739\n",
    "# 4     199399 / 5787630. =  0.03445\n",
    "# 7      37528 / 5787630. =  0.00648\n",
    "# 5      14813 / 5787630. =  0.00256\n",
    "# 6        292 / 5787630. =  0.00005\n",
    "\n",
    "print (len(df_merged.paper_UT.unique())  # 156558   only articles!!  (no reviews, commentaries,corrections,...)\n",
    "       \n",
    "       \n",
    "       \n",
    "       \n",
    "\n",
    "####### get overall avg. and quantiles      \n",
    "\n",
    "list_quantiles_cell=[.25,.5,.75]\n",
    "values_quantiles=list(plos_df['total_refs'].quantile(list_quantiles_cell))     \n",
    "print(\"for plos_df\\n       avg # references included:\",plos_df.total_refs.mean(),  \"  STD:\", plos_df.total_refs.std(), \"   25-50-75:\",values_quantiles)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_references = df_merged.drop_duplicates(subset=['reference_UT'])\n",
    "#print (df_references.shape)\n",
    "\n",
    "\n",
    "list_quantiles_cell=[.25,.5,.75]\n",
    "values_quantiles_cit=list(df_references['cite_count'].quantile(list_quantiles_cell))     \n",
    "values_quantiles_age=list(df_references['diff_year_plos_ref'].quantile(list_quantiles_cell)) \n",
    "print(\"for df_references\\n       avg # citations:\",df_references.cite_count.mean(),  \"  STD:\", df_references.cite_count.std(),  \"  25-50-75:\",values_quantiles_cit,\"\\n       avg age diff:\",df_references.diff_year_plos_ref.mean(),  \"  STD:\", df_references.diff_year_plos_ref.std(),  \"  25-50-75:\",values_quantiles_age)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "list_quantiles_cell=[.25,.5,.75]\n",
    "values_quantiles=list(df_merged['cite_count'].quantile(list_quantiles_cell))#sorted(list(df_select[v1_string].quantile(list_quantiles_cell).to_dict().items()))      \n",
    "print(\"for all df_merged records\\n       avg # citations:\",df_merged.cite_count.mean(),  \"  STD:\", df_merged.cite_count.std(),  \"   25-50-75:\",values_quantiles)\n",
    "\n",
    "\n",
    "\n",
    "                     \n",
    "       \n",
    "       \n",
    "### i look at the groups       \n",
    "df_merged.plos_j1.value_counts() \n",
    "\n",
    "# PLOS ONE       5368044\n",
    "# PLOS GENET      119685\n",
    "# PLO NE TR D      95315\n",
    "# PLOS PATHOG      83451\n",
    "# PLOS COMPUT      65177\n",
    "# PLOS BIOL        44186\n",
    "# PLOS MED         11776\n",
    "# Name: plos_j1, dtype: int64\n",
    "       \n",
    "  \n",
    "       \n",
    "df_merged.plos_field.value_counts()  \n",
    "# ['D RO MULTIDISCIPLINARY SCIENCES']                                                                                                       3779069\n",
    "# ['D CU BIOLOGY']                                                                                                                           880492\n",
    "# ['D RO MULTIDISCIPLINARY SCIENCES', 'D CU BIOLOGY']                                                                                        708479\n",
    "# ['D KM GENETICS & HEREDITY']                                                                                                               119685\n",
    "# ['D YU TROPICAL MEDICINE', 'D TI PARASITOLOGY']                                                                                             95315\n",
    "# ['D ZE VIROLOGY', 'D QU MICROBIOLOGY', 'D TI PARASITOLOGY']                                                                                 83451\n",
    "# ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY']                                                          65072\n",
    "# ['D CQ BIOCHEMISTRY & MOLECULAR BIOLOGY', 'D CU BIOLOGY']                                                                                   44186\n",
    "# ['D PY MEDICINE, GENERAL & INTERNAL']                                                                                                       11776\n",
    "# ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY', 'D PO MATHEMATICS, INTERDISCIPLINARY APPLICATIONS']        105\n",
    "\n",
    "\n",
    "       \n",
    "df_merged.groupby(['plos_field','plos_j1']).size()#.value_counts()  \n",
    "\n",
    "# ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY', 'D PO MATHEMATICS, INTERDISCIPLINARY APPLICATIONS']  PLOS COMPUT        105\n",
    "# ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY']                                                      PLOS COMPUT      65072\n",
    "# ['D CQ BIOCHEMISTRY & MOLECULAR BIOLOGY', 'D CU BIOLOGY']                                                                               PLOS BIOL        44186\n",
    "# ['D CU BIOLOGY']                                                                                                                        PLOS ONE        880494\n",
    "# ['D KM GENETICS & HEREDITY']                                                                                                            PLOS GENET      119685\n",
    "# ['D PY MEDICINE, GENERAL & INTERNAL']                                                                                                   PLOS MED         11776\n",
    "# ['D RO MULTIDISCIPLINARY SCIENCES', 'D CU BIOLOGY']                                                                                     PLOS ONE        708480\n",
    "# ['D RO MULTIDISCIPLINARY SCIENCES']                                                                                                     PLOS ONE       3779070\n",
    "# ['D YU TROPICAL MEDICINE', 'D TI PARASITOLOGY']                                                                                         PLO NE TR D      95315\n",
    "# ['D ZE VIROLOGY', 'D QU MICROBIOLOGY', 'D TI PARASITOLOGY']                                                                             PLOS PATHOG      83451"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (select_data_for_plotting.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary function to select only a subset of the data, according to a number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_data_for_plotting(df_merged, v1_string, years, string_filtering_x, string_references_age, string_isolated_ref, string_self_ref, string_code_categ, string_journal, string_plos_field):\n",
    "    \"\"\"\n",
    "    This function selects a subset of the total dataset acording to a set of parameters.\n",
    "\n",
    "            \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_merged : pandas dataframe\n",
    "        Original dataframe to select from\n",
    "        \n",
    "    v1_string : str\n",
    "        Main variable \n",
    "        \n",
    "    years : list of int\n",
    "        List of selected years\n",
    "        \n",
    "    string_filtering_x : str\n",
    "        Variable for heatmap plot\n",
    "        \n",
    "    string_references_age : str\n",
    "        Selected 'young' or 'old' or 'all' references for the analysis\n",
    "        \n",
    "    string_isolated_ref : str\n",
    "        Selected isolated (1) or group (0) or 'all' references for the analysis\n",
    "        \n",
    "    string_self_ref : str\n",
    "         Selected self references (1) or not self references (0) or 'all' references for the analysis\n",
    "         \n",
    "    string_code_categ : str\n",
    "        Selected PLos category: from 0 to 10 or multiple ones\n",
    "        \n",
    "    string_journal : int\n",
    "        Selected PLoS journal\n",
    "        \n",
    "    string_plos_field : str\n",
    "        Selected PLoS field\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataframe\n",
    "        Selected subset of rows from the original dataset (all columns).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print (\"original size:\",df_merged.shape)\n",
    "\n",
    "\n",
    "\n",
    "    ##### preselection by plos publication year\n",
    "    print (years)\n",
    "    preselection_df = df_merged[df_merged['plos_pub_year'].isin(years)]  \n",
    "    print (\"size of preselection1 (by plos years):\",preselection_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #### i remove self-citations\n",
    "    if (string_self_ref==0) or  ( string_self_ref == 1 ): \n",
    "        preselection_df = preselection_df[preselection_df['self_citation']== string_self_ref ]  \n",
    "        if string_self_ref ==0:\n",
    "            string_self_ref = \", no self-cit\"\n",
    "        elif string_self_ref ==1:\n",
    "            string_self_ref = \", only self-cit\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ######### preselection by isolated or group references:\n",
    "    if (string_isolated_ref==0) or  ( string_isolated_ref == 1 ): \n",
    "        preselection_df0 = preselection_df[preselection_df['isolated_citation']== string_isolated_ref ]  \n",
    "\n",
    "        if string_isolated_ref ==0:\n",
    "            string_isolated_ref = \", group ref\"\n",
    "        elif string_isolated_ref ==1:\n",
    "            string_isolated_ref = \", isolated ref\"\n",
    "    else:    \n",
    "        preselection_df0 = preselection_df   \n",
    "        print (\"size of preselection1 (by isolated/group ref):\",preselection_df0.shape, string_isolated_ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ######### preselection by plos ONE subject category:\n",
    "    if string_code_categ==\"\": \n",
    "        preselection_df111 = preselection_df0\n",
    "    else:    \n",
    "        if \" \" not in string_code_categ:  # to include one single category\n",
    "            preselection_df111 = preselection_df0[preselection_df0['categ_codes'].str.contains(string_code_categ)]        \n",
    "            string_code_categ = \" \"+dict_code_categ[string_code_categ]  \n",
    "\n",
    "        else:  # if multiple codes-categories\n",
    "            list_codes = string_code_categ.split(\" \")\n",
    "            print (list_codes)\n",
    "\n",
    "            if len(list_codes) >= 2:              \n",
    "                preselection_df111 = preselection_df0[ preselection_df0['categ_codes'].str.contains('|'.join(list_codes)) ]  # to look for partial matches from a list of strings!!!!!\n",
    "\n",
    "\n",
    "            string_code_categ = \"\" \n",
    "            for code in list_codes:\n",
    "                string_code_categ += \"-\"+dict_code_categ[code] \n",
    "\n",
    "\n",
    "        print (\" size of preselection (by plos ONE subject category):\",preselection_df111.shape, string_code_categ)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ######### preselection by plos journal:\n",
    "    if string_journal==\"\": \n",
    "        preselection_df1 = preselection_df111\n",
    "    else:    \n",
    "        preselection_df1 = preselection_df111[preselection_df111['plos_j1']== string_journal ]  \n",
    "    print (\" size of preselection2 (by plos journal):\",preselection_df1.shape, string_journal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ######### preselection by plos field:\n",
    "    if string_plos_field==\"\": \n",
    "        preselection_df2 = preselection_df1\n",
    "    else:    \n",
    "        preselection_df2 = preselection_df1[preselection_df1['plos_field']== string_plos_field ]  \n",
    "    print (\" size of preselection2 (by plos field):\",preselection_df2.shape, string_plos_field)\n",
    "\n",
    "\n",
    "    preselection_df3 = preselection_df2\n",
    "\n",
    "    if v1_string ==  'cite_count'  or       v1_string ==  'log_num_cit_ref'   or v1_string == 'log2_num_cit_ref':\n",
    "\n",
    "        string_age_selv1_stringection=''\n",
    "\n",
    "        ##### preselection only young/old references:        \n",
    "        if string_references_age == \"young\":\n",
    "            time_window = 1\n",
    "            string_age_selection=\"only young references from >=\"+ str((min(years)-time_window))\n",
    "            preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] >= (min(years)-time_window) ]   \n",
    "            print (\"  size of preselection3 (only young references):\",preselection_df3.shape, string_age_selection)\n",
    "\n",
    "        elif string_references_age == \"old\":\n",
    "            time_window = 10\n",
    "            string_age_selection=\"only old references from <=\"+str((min(years)-time_window))\n",
    "            preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] <= (min(years)-time_window) ]   \n",
    "            print (\"  size of preselection3 (only young references):\",preselection_df3.shape,string_age_selection )\n",
    "\n",
    "        else:\n",
    "            string_age_selection=\"young&old\"       \n",
    "            print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return preselection_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIGURE: Multiplot for barplots of total number of records and total number of papers per journal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select the the subset of data for plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "v1_string =  'cite_count'\n",
    "\n",
    "years=[2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017]   #### SELECT THE YEARS TO BE INCLUDED\n",
    "\n",
    "string_filtering_x = 'paper_cite_count'   \n",
    "\n",
    "string_references_age = \"\"   # AGE OF REFERENCES TO BE INCLUDED:  young     old   \n",
    "\n",
    "string_isolated_ref = \"\"   # WHETHER OR NOT TO INCLUDE ISOLATED REFERENCES:   # 0  or 1 (or empty string, to include all ref)\n",
    "\n",
    "string_self_ref =0    #  WHETHER OR NOT TO INCLUDE SELF-REFERENCES:  #     0  or 1 (or empty string, to include all ref)\n",
    "\n",
    "\n",
    "string_code_categ=\"\" #  plos ONE categories:  the codes are strings (0  TO 10), not integers. if i want to include multiple subjects:  \"1 2 8\"\n",
    "\n",
    "string_journal=\"\"   #  plos journals \n",
    "\n",
    "string_plos_field=\"\"  #  WoS subject categories. \n",
    "\n",
    "\n",
    "\n",
    "preselection_df3 = select_data_for_plotting(df_merged, v1_string, years, string_filtering_x, string_references_age, string_isolated_ref, string_self_ref, string_code_categ, string_journal, string_plos_field)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print (\"\\nTot # records included:\",len(preselection_df3),\"   # number of plos papers:\",len(preselection_df3.paper_UT.unique()), \"   # unique ref:\", len(preselection_df3.reference_UT.unique()),'\\n')\n",
    "\n",
    "####################################\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I create the (empty) multiplot to be populated with the different subplots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.001, horizontal_spacing=0.001)\n",
    "    \n",
    "    \n",
    "list_colors = [\"#0000FF\",\"#FF4040\",\"#4EEE94\",\"#87CEFA\",\"#FFA500\",\"#EE82EE\",\"#8B8B83\"]\n",
    "list_journals = ['PLOS ONE', 'PLOS GENET', 'PLO NE TR D','PLOS PATHOG', 'PLOS COMPUT', 'PLOS BIOL',  'PLOS MED']\n",
    "\n",
    "\n",
    "\n",
    "data = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I populate the multiplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "### FOR THE NUMBER OF PAPERS\n",
    "cont = 0\n",
    "for journal in list_journals:\n",
    "    df_selection = preselection_df3[preselection_df3['plos_j1'] == journal]        \n",
    "    \n",
    "    trace1 = go.Bar(\n",
    "        x=['# papers'],\n",
    "        y=[ len(df_selection.paper_UT.unique())],\n",
    "        marker=dict( color=list_colors[cont]),\n",
    "        name=journal.replace('PLO NE TR D','Neglected<br>  Tropical Diseases').replace('PLOS BIOL','Biology').replace('PLOS COMPUT','Computational<br>  Biology').replace('PLOS GENET','Genetics').replace('PLOS MED','Medicine').replace('PLOS PATHOG','Pathology'),\n",
    "        showlegend=True\n",
    "    )\n",
    "   \n",
    "    fig.append_trace(trace1, 1, 1)\n",
    "    cont +=1\n",
    "\n",
    "\n",
    "    \n",
    "### FOR THE NUMBER OF RECORDS    \n",
    "cont = 0    \n",
    "for journal in list_journals:\n",
    "    df_selection = preselection_df3[preselection_df3['plos_j1'] == journal]\n",
    "    \n",
    "    \n",
    "    trace2 = go.Bar(\n",
    "        x=['# records'],   \n",
    "        y=[len(df_selection)],\n",
    "        marker=dict( color=list_colors[cont]),\n",
    "        showlegend=False,\n",
    "        xaxis='x2',\n",
    "        yaxis='y2'\n",
    "    )\n",
    "    fig.append_trace(trace2, 1, 2)\n",
    "    cont  +=1\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I customize the layout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "font_gral=55 # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "\n",
    "\n",
    "fig['layout'].update( legend=dict(x=1.2, y=1.0), margin=go.Margin( l=250))#, b=150),)# r=50, b=150,  #t=100, # pad=4 ), )\n",
    "fig['layout'].update(barmode='stack')\n",
    "#fig['layout']['yaxis'].update(type='log')\n",
    "#fig['layout']['yaxis'].update(range=[0, 5.3])  ### ojo!! rango logaritmico!\n",
    "#fig['layout']['yaxis2'].update(type='log')\n",
    "#fig['layout']['yaxis'].update(range=[0, 6.84])  ### ojo!! rango logaritmico!\n",
    "fig['layout']['yaxis2'].update(side='right')\n",
    "fig['layout']['yaxis'].update(title='Count')\n",
    "fig['layout']['font']['size'] = font_gral\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# axis\n",
    "fig['layout']['xaxis']['tickfont']['size']  = font_gral + 10\n",
    "fig['layout']['xaxis2']['tickfont']['size'] = font_gral + 10\n",
    "fig['layout']['yaxis']['tickfont']['size']  = font_gral + 10\n",
    "fig['layout']['yaxis2']['tickfont']['size'] = font_gral + 10\n",
    "fig['layout']['xaxis'].update(domain=[0, 0.45]) # this is to force the relative location of the two panels with respect to each other\n",
    "fig['layout']['yaxis'].update(domain=[0, 1])\n",
    "fig['layout']['xaxis2'].update( domain=[0.55, 1])\n",
    "fig['layout']['yaxis2'].update(domain=[0, 1],  anchor='x2')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "offline.plot(fig, auto_open=True, image = 'png', image_filename=\"fig1c\" ,image_width=2000, image_height=1000, \n",
    "              filename='/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/fig1c', validate=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would be the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/Barplot_Number_papers_number_records.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIGURE CAPTION: number of papers and number of records in our dataset for each one of the different PLoS journals, and it includes all publication years together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIGURE:  Heatmap plot for median age of referenceds used, by impact category of the citing paper, and by paper section.  \n",
    "## Next, I'll also run pairwise statistical comparisons between all pairs of cells in a given heatmap, to determine whether the observed differences are significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I select the subset of data for plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "v1_string =  'diff_year_plos_ref' \n",
    "string_references_age = \"\"   # AGE OF REFERENCES TO BE INCLUDED:  young     old   \n",
    "      \n",
    "    \n",
    "if v1_string ==  'cite_count'  :\n",
    "    colorbar_string = 'Citations'\n",
    "    fig_colorscale = \"Reds\"\n",
    "    fig_font_colors = ['#ff0000', '#ffece6']\n",
    "else:\n",
    "    colorbar_string = 'Age [yr]' \n",
    "    fig_colorscale = \"Greens\"\n",
    "    fig_font_colors = ['#205803', '#dcf0d2'] \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "years = [2009]   #### SELECT THE YEARS TO BE INCLUDED\n",
    "\n",
    "string_filtering_x = 'paper_cite_count'   \n",
    "\n",
    " \n",
    "\n",
    "string_isolated_ref = \"\"   # WHETHER OR NOT TO INCLUDE ISOLATED REFERENCES:   # 0  or 1 (or empty string, to include all ref)\n",
    "\n",
    "string_self_ref =0    #  WHETHER OR NOT TO INCLUDE SELF-REFERENCES:  #     0  or 1 (or empty string, to include all ref)\n",
    "\n",
    "\n",
    "string_code_categ=\"\" #  plos ONE categories:  the codes are strings (0  TO 10), not integers. if i want to include multiple subjects:  \"1 2 8\"\n",
    "\n",
    "string_journal=\"\"   #  plos journals \n",
    "\n",
    "string_plos_field=\"\"  #  WoS subject categories. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preselection_df3 = select_data_for_plotting(df_merged, v1_string, years, string_filtering_x, string_references_age, string_isolated_ref, string_self_ref, string_code_categ, string_journal, string_plos_field)\n",
    "print (\"\\nTot # records included:\",len(preselection_df3),\"   # number of plos papers:\",len(preselection_df3.paper_UT.unique()), \"   # unique ref:\", len(preselection_df3.reference_UT.unique()),'\\n')\n",
    "\n",
    "####################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dict_group_subset_data={}\n",
    "dict_group_quantiles_size={}\n",
    "\n",
    " \n",
    "  \n",
    "######## i get the bins number of citation of the plos papers: i want the same bins for all papers (so i calculate them before separating into sections but after all the preselections)\n",
    "\n",
    "list_q=[0.3,0.6,.9,.99,1]    # for the percentile sections for number of citations of the PLOS papers\n",
    "   \n",
    "quantiles=sorted(list(preselection_df3[string_filtering_x].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    "\n",
    "\n",
    "lista_bins_plos_citations=[]\n",
    "old_value=0\n",
    "for item in quantiles:\n",
    "    try:\n",
    "        pair=[old_value, int(item[1])]    \n",
    "    except:  # if it is a nan:\n",
    "        pair=[old_value, item[1]]\n",
    "\n",
    "    lista_bins_plos_citations.append(pair)\n",
    "\n",
    "    try:\n",
    "        old_value = int(item[1])\n",
    "    except:\n",
    "        old_value = item[1]\n",
    "\n",
    "print (lista_bins_plos_citations)\n",
    "\n",
    "################################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I create the grouping of data corresponding to the different cells of the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "lista_titulos_sets = []\n",
    "lista_sections = [\"Introduction\",\"Methods\",\"Results\",\"Discussion\"]\n",
    "\n",
    "cont=0\n",
    "for item in lista_bins_plos_citations:\n",
    "\n",
    "    minimo = item[0]\n",
    "    maximo = item[1]\n",
    "\n",
    "\n",
    "    preselection_df4 = preselection_df3[(preselection_df3[string_filtering_x] >= minimo)  &  (preselection_df3[string_filtering_x] < maximo)]  \n",
    "\n",
    "    x1_All = list(preselection_df4[v1_string])\n",
    "\n",
    "\n",
    "\n",
    "    for string_section in lista_sections:\n",
    "\n",
    "\n",
    "        ##### preselection to include only occurrences in a section of the paper\n",
    "        if  string_section == \"Introduction\":\n",
    "            section=0\n",
    "        elif  string_section == \"Methods\":\n",
    "            section=1\n",
    "        elif  string_section == \"Results\":\n",
    "            section=2\n",
    "        elif  string_section == \"Discussion\":\n",
    "            section=3\n",
    "\n",
    "\n",
    "\n",
    "        df_select = preselection_df4[preselection_df4['regex_sect_index']== section]   \n",
    "        \n",
    "\n",
    "        x1 = list(df_select[v1_string])       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if cont ==0:            \n",
    "            group=string_section+\" Bottom\" \n",
    "        elif cont ==1:            \n",
    "                   \n",
    "            group=string_section+\" Typical\"       \n",
    "        elif cont==2:\n",
    "                 \n",
    "             group=string_section+\" Good\"    \n",
    "        elif cont==3: \n",
    "               \n",
    "            group=string_section+\" High\"\n",
    "        elif cont==4:\n",
    "                 group=string_section+\" Top\"\n",
    "            \n",
    "            \n",
    "        lista_titulos_sets.append(group)    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ######### i get also quantiles for each cell:    \n",
    "        list_quantiles_cell=[.25,.5,.75]\n",
    "\n",
    "        values_quantiles=list(df_select[v1_string].quantile(list_quantiles_cell))#sorted(list(df_select[v1_string].quantile(list_quantiles_cell).to_dict().items()))      \n",
    "\n",
    "        tupla=values_quantiles + [len(x1)]\n",
    "\n",
    "        dict_group_quantiles_size[group] = tupla\n",
    "\n",
    "        dict_group_subset_data[group]=x1\n",
    "\n",
    "\n",
    "\n",
    "    cont +=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################ i also add the median values for the section across all data in the preselection\n",
    "for string_section in lista_sections:\n",
    "\n",
    "\n",
    "    if  string_section == \"Introduction\":\n",
    "        section=0\n",
    "    elif  string_section == \"Methods\":\n",
    "        section=1\n",
    "    elif  string_section == \"Results\":\n",
    "        section=2\n",
    "    elif  string_section == \"Discussion\":\n",
    "        section=3\n",
    "\n",
    "\n",
    "    df_select = preselection_df3[preselection_df3['regex_sect_index']== section]   \n",
    "\n",
    "    list_quantiles_cell=[.25,.5,.75]\n",
    "    values_quantiles=list(df_select[v1_string].quantile(list_quantiles_cell))  \n",
    "    tupla=values_quantiles + [len(df_select)]\n",
    "\n",
    "\n",
    "\n",
    "########  I create the x, y, z lists of values for the heatmap\n",
    "\n",
    "lista_y=lista_sections\n",
    "\n",
    "lista_bin_names=[\" Bottom\",\" Typical\",\" Good\",\" High\",\" Top\"]\n",
    "\n",
    "lista_x=lista_bin_names\n",
    "\n",
    "lista_z25=[]\n",
    "lista_z50=[]\n",
    "lista_z75=[]\n",
    "lista_z_sizes=[]\n",
    "\n",
    "for x_value in lista_x:    \n",
    "    aux_lista25=[]\n",
    "    aux_lista50=[]\n",
    "    aux_lista75=[]\n",
    "    aux_lista_sizes=[]\n",
    "\n",
    "    for y_value in lista_y:       \n",
    "\n",
    "        llave=y_value+x_value\n",
    "\n",
    "        try:\n",
    "            value=int(dict_group_quantiles_size[llave][0])\n",
    "        except:  # if it is a nan:\n",
    "            value=dict_group_quantiles_size[llave][0]\n",
    "        aux_lista25.append(value)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        try:\n",
    "            value=int(dict_group_quantiles_size[llave][1])\n",
    "        except:  # if it is a nan:\n",
    "            value=dict_group_quantiles_size[llave][1]\n",
    "        aux_lista50.append(value)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        try:\n",
    "            value=int(dict_group_quantiles_size[llave][2])\n",
    "        except:  # if it is a nan:\n",
    "            value=dict_group_quantiles_size[llave][2]\n",
    "        aux_lista75.append(value)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        value_size=dict_group_quantiles_size[llave][3]\n",
    "        aux_lista_sizes.append(value_size)\n",
    "\n",
    "\n",
    "        #print (y_value,\" \",x_value, value, value_size)\n",
    "    lista_z25.append(aux_lista25)\n",
    "    lista_z50.append(aux_lista50)\n",
    "    lista_z75.append(aux_lista75)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    lista_z_sizes.append(aux_lista_sizes)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "######## I get a customized text for each cell\n",
    "lista_text_z=[]\n",
    "for i in range(len(lista_z_sizes)):\n",
    "    aux=[]\n",
    "    for j in range(len(lista_z_sizes[0])):        \n",
    "        value=str(lista_z25[i][j])+\"-<b>\"+str(lista_z50[i][j])+\"</b>-\"+str(lista_z75[i][j])+\"<br>(\"+str(format(lista_z_sizes[i][j], ',d'))+\")\"           \n",
    "\n",
    "        aux.append(value)\n",
    "        \n",
    "    lista_text_z.append(aux)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  color selection and titles for the figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_space = 150\n",
    "if v1_string ==  'cite_count'  :\n",
    "    colorbar_string = 'Citations'\n",
    "    if string_references_age == \"old\" :\n",
    "        colorbar_string = ''\n",
    "else:\n",
    "    colorbar_string = 'Age [yr]'\n",
    "    top_space = 100\n",
    "    text_abc = '(b)'\n",
    "\n",
    "    \n",
    "fig_font_colors=''\n",
    "\n",
    "\n",
    "factor_color_rescale =.6  \n",
    "\n",
    "fig_colorscale=[[0.0*factor_color_rescale, '#ffffff'],\\\n",
    "                       [0.1*factor_color_rescale, '#d9f2d9'],\\\n",
    "                       [0.2*factor_color_rescale, '#c6ecc6'],\\\n",
    "                       [0.3*factor_color_rescale, '#b3e6b3'],\\\n",
    "                       [0.4*factor_color_rescale, '#8cd98c'], \\\n",
    "                       [0.5*factor_color_rescale, '#66cc66'], \\\n",
    "                       [0.6*factor_color_rescale, '#53c653'], \\\n",
    "                       [0.7*factor_color_rescale, '#40bf40'],\\\n",
    "                       [0.75*factor_color_rescale, '#39ac39'],\\\n",
    "                       [0.8*factor_color_rescale, '#339933'],\\\n",
    "                       [0.85*factor_color_rescale, '#2d862d'],\\\n",
    "                       [0.9*factor_color_rescale, '#267326'],\\\n",
    "                      [1.0, '#000000']]\n",
    "\n",
    "\n",
    "\n",
    "fig_font_colors = ['#205803', '#dcf0d2']      # same for the annotation of the boxes (to make sure they are readable)\n",
    "fig_filename = '../plots/annotated-heatmap_median_age_difference_plos_publ_year_vs_references_for_sections_and_subsect_by_citations_of_plos'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I create the figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = ff.create_annotated_heatmap(z=lista_z50, x=lista_sections, y=lista_bin_names, annotation_text=lista_text_z, colorscale=fig_colorscale, font_colors=fig_font_colors,showscale=True, colorbar=dict(title=colorbar_string, titleside='right' ),)#, reversescale=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig.layout.title = \"\"# fig_title_plot\n",
    "\n",
    "fig['layout']['xaxis']['side'] = 'bottom'\n",
    "fig.layout.xaxis.update({'title': 'Section'})\n",
    "\n",
    "\n",
    "fig.layout.yaxis.update({'title': 'Impact Group'})\n",
    "if v1_string ==  'cite_count'  :\n",
    "    if string_references_age == \"young\":  \n",
    "        fig.layout.yaxis.update({'title': ''})\n",
    "\n",
    "font_gral=25  # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "fig['layout']['font']['size'] = font_gral\n",
    "\n",
    "      \n",
    "if v1_string ==  'cite_count'  :\n",
    "    if string_references_age == \"young\":  \n",
    "        #fig.layout.update({'title': '$d, r \\\\text{ (solar radius)}$'})\n",
    "        fig['layout']['title'] = \"Young references\"\n",
    "    elif string_references_age == \"old\":  \n",
    "        fig.layout.update({'title': 'Old references'})\n",
    "\n",
    "    fig.layout.update({'font': dict(size=25)})\n",
    "\n",
    "\n",
    "font_gral=55  # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "fig['layout']['font']['size'] = font_gral\n",
    "\n",
    "\n",
    "# axis\n",
    "\n",
    "fig['layout']['xaxis']['tickangle'] = 0\n",
    "fig['layout']['yaxis']['tickangle'] = -90\n",
    "fig['layout']['xaxis']['titlefont']['size'] = font_gral + 20\n",
    "fig['layout']['yaxis']['titlefont']['size'] = font_gral\n",
    "fig['layout']['xaxis']['tickfont']['size'] = font_gral -7 \n",
    "fig['layout']['yaxis']['tickfont']['size'] = font_gral -15\n",
    "fig['layout']['margin']=dict(\n",
    "        l=200,\n",
    "       # r=50,\n",
    "        b=150,\n",
    "        t=top_space,\n",
    "        pad=15\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline.plot(fig, auto_open=True, image = 'png', image_filename=fig_filename ,image_width=2000, image_height=1200, filename=fig_filename+'.html', validate=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This would be the result:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/Heatmap_plot_age_of_references_by_section_and_paper_impact_group.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIGURE CAPTION (THIS RESULTS ARE STILL UNPUBLISHED, PLEASE KEEP IT CONFIDENTIAL): The references used in the Methods section of  PLoS papers are the oldest, while the ones used in the Discussion section are the youngest. We also observe how the higher the (future) impact of the citing paper, the youngers the references it uses, accross all paper sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Next, I need to run tests for the statistical comparison of all pairs of sub-sets (cells) displayed in the previous heatmap plot. Given that the original heatmap plot has 4x5 = 20 cells, that means I have to run N(N-1)/2  pairwise comparisons (to create and structure the plot more easily, I will repeat and replot the comparisons i-j  and j-i).\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select what statistical test to run: KS (Kolmogorovâ€“Smirnov) to test if the distributions are different or  MW (mann whitney u test) for testing just whether the medians are different from each other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"MW\"  # MW or KS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I apply the bonferroni correction, so my new p-value (p_value_threshold)  required for significance is: old_p_value /Number of comparisons = 0.001 / (20*19/2)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    \n",
    "list_keys_macro = ['Introduction Top',    'Methods Top',    'Results Top',   'Discussion Top', \\\n",
    "                   'Introduction High',   'Methods High',  'Results High',  'Discussion High', \\\n",
    "                   'Introduction Good',   'Methods Good',   'Results Good',  'Discussion Good', \\\n",
    "                   'Introduction Typical', 'Methods Typical', 'Results Typical', 'Discussion Typical', \\\n",
    "                   'Introduction Bottom',  'Methods Bottom',  'Results Bottom', 'Discussion Bottom']\n",
    "    \n",
    "      \n",
    "list_keys_heatmap = ['Introduction Bottom', 'Methods Bottom',  'Results Bottom', 'Discussion Bottom',\\\n",
    "                     'Introduction Typical', 'Methods Typical', 'Results Typical', 'Discussion Typical',\\\n",
    "                     'Introduction Good',   'Methods Good',   'Results Good',  'Discussion Good', \\\n",
    "                     'Introduction High',   'Methods High',  'Results High',  'Discussion High', \\\n",
    "                     'Introduction Top',   'Methods Top',    'Results Top',   'Discussion Top'  ]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "   \n",
    "\n",
    "lista_indeces = [[1,1],[1,2],[1,3],[1,4],\\\n",
    "                 [2,1],[2,2],[2,3],[2,4],\\\n",
    "                 [3,1],[3,2],[3,3],[3,4],\\\n",
    "                 [4,1],[4,2],[4,3],[4,4],\\\n",
    "                 [5,1],[5,2],[5,3],[5,4]]\n",
    "\n",
    "\n",
    "\n",
    "threshold_zero = 0.0001 / (float(len(list_keys_macro))*float(len(list_keys_macro)-1)/2.)    # to round up to zero the very small p_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### I calculate the p-values corresponding to each pairwise comparison between cells, and i directly mark them as significant / non-significant (after correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lista_tot_datos=[]\n",
    "\n",
    "total_cont =0\n",
    "for i in list_keys_macro:\n",
    "    lista_listas=[]\n",
    "    aux_lista=[]\n",
    "    cont=1\n",
    "   \n",
    "    for j in list_keys_heatmap:\n",
    "             \n",
    "        set1 = dict_group_subset_data[i]\n",
    "        set2 = dict_group_subset_data[j]\n",
    "        \n",
    "        if test == \"KS\":\n",
    "            p_value = stats.ks_2samp(set1, set2)[1] \n",
    "        elif test == \"MW\":\n",
    "            p_value = stats.mannwhitneyu(set1, set2,  alternative='two-sided')[1]  \n",
    "        \n",
    "        \n",
    "        if p_value <= threshold_zero:  #i round up to zero the very small p_values\n",
    "                p_value =0.\n",
    "                \n",
    "        else:\n",
    "            p_value = .5  ### I ONLY CARE ABOUT WHETHER IT IS SIGNIFICANT OR NOT, I DONT CARE ABOUT THE EXACT P-VALUE ONCE IT IS NOT SIGNIFICANT\n",
    "                \n",
    "        if i == j:  # i single out manually the self comparison  (to color it differently later)\n",
    "            p_value=1.001  \n",
    "            \n",
    "            \n",
    "            \n",
    "        aux_lista.append(p_value)\n",
    "        \n",
    "        cont +=1\n",
    "        \n",
    "        if cont > tot_cols:\n",
    "            lista_listas.append(aux_lista)                  \n",
    "            aux_lista=[]\n",
    "            cont=1\n",
    "            \n",
    "    \n",
    "    \n",
    "    total_cont +=1\n",
    "    lista_tot_datos.append(lista_listas)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    "#### I create an empty plot and I populate it with the different pairwise comparisons\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig_colorscale=  [ [0., '#0059b3'], [.5,'#c7dcf1'], [1.,'#bdbdbd']] #  0 or anything significant: blue,   .5 or anithing NON signif: light-blue,     1: grey\n",
    "lista_bin_names = [\"Bottom\",\"Typical\",\"Good\",\"High\",\"Top\"]\n",
    "lista_sections  =  ['Intro', 'Methods', 'Results', 'Discussion']\n",
    "\n",
    "\n",
    "tot_rows = 5\n",
    "tot_cols = 4  \n",
    "    \n",
    "      \n",
    "\n",
    "\n",
    "fig_macro = None\n",
    "fig_macro = tools.make_subplots(rows=tot_rows, cols=tot_cols, shared_xaxes=True, shared_yaxes=True, vertical_spacing = 0.01, horizontal_spacing = 0.01,   )\n",
    "\n",
    "\n",
    "for i in range(len(lista_tot_datos)):\n",
    "    datos = lista_tot_datos[i]\n",
    "    \n",
    "    cont_rows = lista_indeces[i][0]\n",
    "    cont_cols = lista_indeces[i][1]\n",
    "    \n",
    "    trace1 = go.Heatmap(z=datos,\n",
    "                       x=lista_sections,\n",
    "                       y=lista_bin_names,                        \n",
    "                       colorscale = fig_colorscale,\n",
    "                       showscale=False,)\n",
    "                        #reversescale=True, )#    )\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # i add each individual plot\n",
    "    fig_macro.append_trace(trace1, cont_rows, cont_cols)\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fontsize=32 \n",
    "fig_macro['layout']['font'].update({'size': fontsize})\n",
    "\n",
    "\n",
    "lista_bin_names = [\"Bottom\",\"Typical\",\"Good\",\"High\",\"Top\"]\n",
    "lista_sections  =  ['Introduction', 'Methods', 'Results', 'Discussion']\n",
    "\n",
    "\n",
    "# AXIS\n",
    "fig_macro['layout']['xaxis1'].update(title=lista_sections[0])  \n",
    "fig_macro['layout']['xaxis2'].update(title=lista_sections[1])\n",
    "fig_macro['layout']['xaxis3'].update(title=lista_sections[2])\n",
    "fig_macro['layout']['xaxis4'].update(title=lista_sections[3])\n",
    "\n",
    "\n",
    "fig_macro['layout']['yaxis1'].update(title=lista_bin_names[4])   \n",
    "fig_macro['layout']['yaxis2'].update(title=lista_bin_names[3])\n",
    "fig_macro['layout']['yaxis3'].update(title=lista_bin_names[2])\n",
    "fig_macro['layout']['yaxis4'].update(title=lista_bin_names[1])\n",
    "fig_macro['layout']['yaxis5'].update(title=lista_bin_names[0])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline.plot(fig_macro, auto_open=True, image = 'png', image_filename='multiplot_comparisons' ,image_width=3000, image_height=2200,\n",
    "              filename='../plots/multiplot_comparisons.html', validate=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This would be the result:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/multiplot_pairwise_comparisons_WM_test_for_age_of_references.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIGURE CAPTION (THIS RESULTS ARE UNPUBLISHED YET, PLEASE KEEP IT CONFIDENTIAL): We display the results from all pairwise comparison between cells from the previous figure (the GREEN one). \n",
    "Dark blue represents significantly different (after correction for multiple testing), light blue corresponds to non-significant, and grey marks a self-comparison. Most of the pairwise comparisons between cells are statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of a null model to compare the actual vs expected usage of top and bottom references by top and bottom PLoS papers.\n",
    "## This randomization scheme controls for plos subject field and PLoS publication year, and it also preserves clusters of references used together in a section of a given paper.   \n",
    "## This code does the randomization procces for the selected subset of data (Niter independent randomizations) and plots actual vs expected values, as well as provides z-scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary function needed to implement the randomization scheme that preserves groups/clusters of references that are cited together in  a given paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_lists_references(preselection_df3):\n",
    "\n",
    "    \"\"\"\n",
    "    This function provides a list of lists of references used by all PLOS papers in the selected sub set of data, \n",
    "    preserving those references that appear together in a cluster in a section of a given paper\n",
    "            \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    preselection_df3 : pandas dataframe\n",
    "        \n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of lists with the IDs of the references\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "    ####  NOTE: preselection_df3 only includes one instance of paper_UT-ref_UT  (the first occurrence in each paper), and i sort it too:\n",
    "    preselection_df3.sort_values(by=['paper_UT','reference_UT'], inplace=True)\n",
    "\n",
    "\n",
    "    distance_threshold = 5  # characters tops to separate group ref\n",
    "\n",
    "    list_lists_all_ref = []  # WITH STRUCTURE\n",
    "    lista_ref = []           # WITHOUT STRUCTURE\n",
    "    cont = 0\n",
    "    for paper_UT, group_df in preselection_df3.groupby(['paper_UT']):  #### OJO!!!! THIS LOOP IS WAY FASTER THAN DOING:  for   paper_UT in list_paper_UT    !!!!    \n",
    "\n",
    "        group_df.sort_values(by=['regex_sect_index','sect_char_pos','reference_UT'],inplace = True)  # i sort the reference of a paper by section first, then by location within the section\n",
    "\n",
    "\n",
    "\n",
    "        ### first i take care of the isolated references: \n",
    "        group_df1= group_df[group_df['isolated_citation'] ==1]        \n",
    "        for index, row in group_df1.iterrows():  \n",
    "\n",
    "            ref_UT = row['reference_UT']\n",
    "            aux = [ref_UT]\n",
    "            lista_ref.append(ref_UT)   \n",
    "            list_lists_all_ref.append(aux)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #### then i take care of the group references:\n",
    "        group_df0= group_df[group_df['isolated_citation'] == 0]  \n",
    "        previous_position = 0\n",
    "        list_group_ref = []\n",
    "        for index, row in group_df0.iterrows():\n",
    "\n",
    "            ref_UT = row['reference_UT']\n",
    "            lista_ref.append(ref_UT)   # list without structure (for comparison reasons)\n",
    "\n",
    "            position = row['sect_char_pos']\n",
    "\n",
    "            if previous_position == 0:  # for the very first entry                \n",
    "                list_group_ref.append(ref_UT)\n",
    "\n",
    "\n",
    "            else:  # for all other entries\n",
    "                if (position - previous_position) <= distance_threshold  :   # if the current ref is close to the previous one\n",
    "                     list_group_ref.append(ref_UT)\n",
    "                else: \n",
    "                    list_lists_all_ref.append(list_group_ref)\n",
    "                    list_group_ref = []\n",
    "                    list_group_ref.append(ref_UT)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            previous_position = position\n",
    "\n",
    "        list_lists_all_ref.append(list_group_ref)  # i need this for the final group/isolated one!!!\n",
    "\n",
    "        cont +=1   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #### i flatten out the list of lists for comparison purposes\n",
    "\n",
    "    flat_list = []    \n",
    "    for sublist in list_lists_all_ref:\n",
    "        for item in sublist:\n",
    "            flat_list.append(item)\n",
    "\n",
    "    print (\"list_lists created:\", len(list_lists_all_ref), ' without structure:', len(lista_ref), \"   flat_list:\", len(flat_list))\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    return list_lists_all_ref\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the values of the parameters for the selection of the data, as well and number of iterations for the randomization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Niter=1000   ##### number of iterations of the randomization process\n",
    "\n",
    "years=[2010]  # controlling by year\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "########### select the subset of data for plotting:\n",
    "v1_string = 'regex_sect_index'\n",
    "       \n",
    "\n",
    "string_filtering_x = 'paper_cite_count'   \n",
    "\n",
    "string_references_age = \"all\"   # AGE OF REFERENCES TO BE INCLUDED:  young     old   \n",
    "\n",
    "string_isolated_ref = \"\"   # WHETHER OR NOT TO INCLUDE ISOLATED REFERENCES:   # 0  or 1 (or empty string, to include all ref)\n",
    "\n",
    "string_self_ref =0    #  WHETHER OR NOT TO INCLUDE SELF-REFERENCES:  #     0  or 1 (or empty string, to include all ref)\n",
    "\n",
    "\n",
    "string_code_categ=\"\" #  plos ONE categories:  the codes are strings (0  TO 10), not integers. if i want to include multiple subjects:  \"1 2 8\"\n",
    "\n",
    "string_journal=\"\"   #  plos journals \n",
    "\n",
    "string_plos_field=\"\"  #  WoS subject categories. \n",
    "\n",
    "\n",
    "### PLoS subject category\n",
    "string_code_categ = '0' #,'1'#,'2','3','4','5','6','7','8','9','10']  # I control by PLOS field: i run the randomizations independently for each one\n",
    "\n",
    "######################3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preselection_df3 = select_data_for_plotting(df_merged, v1_string, years, string_filtering_x, string_references_age, string_isolated_ref, string_self_ref, string_code_categ, string_journal, string_plos_field) \n",
    "\n",
    "\n",
    "\n",
    "#### for the randomization, i only count each reference once per paper!!\n",
    "preselection_df3 = preselection_df3.drop_duplicates(subset=['paper_UT', 'reference_UT'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "N_plos = len(preselection_df3.paper_UT.unique())         \n",
    "N_ref = len(preselection_df3.reference_UT.unique()) \n",
    "N_all = len(preselection_df3)\n",
    "print (\"     N plos:\", N_plos,\"  N  ref:\",N_ref, \" N records:\", N_all)        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This randomization will compare top vs bottom categories of either papers or references ('top' and 'bottom' defined as the percentiles of number of citations accrued), so I need to get the corresponding data for the percentiles in both cases.\n",
    "#### First, I get the data corresponding to the percentiles for the PLoS papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "############## i define quantiles for plos papers and for references (\"TOP\" and \"BOTTOM\") \n",
    "list_q_plos=[.1,.9,1]\n",
    "list_q_ref=[.1,.9,1]\n",
    "\n",
    "\n",
    "\n",
    "df_for_quantiles_plos = preselection_df3.drop_duplicates(subset=['paper_UT'])  \n",
    "\n",
    "quantiles=sorted(list(df_for_quantiles_plos['paper_cite_count'].quantile(list_q_plos).to_dict().items())) \n",
    "print (\"\\n\\ncitation bins for the selected plos:\", list_q_plos) \n",
    "\n",
    "lista_bins_plos=[]\n",
    "old_value=0\n",
    "for item in quantiles:\n",
    "    pair=[old_value, int(item[1])]\n",
    "    lista_bins_plos.append(pair)\n",
    "    old_value = int(item[1])   \n",
    "\n",
    "print (\"\\nbins for PLOS papers:\")\n",
    "\n",
    "cont = 0\n",
    "dict_bin_list_plos_UT={}\n",
    "for item in lista_bins_plos:\n",
    "\n",
    "    minimo = item[0]\n",
    "    maximo = item[1]   \n",
    "\n",
    "    df_select = preselection_df3[(preselection_df3['paper_cite_count'] >= minimo)  &  (preselection_df3['paper_cite_count'] < maximo)]\n",
    "    llave=str(minimo)+\"-\"+str(maximo)\n",
    "    dict_bin_list_plos_UT[llave]= list(df_select.paper_UT.unique())\n",
    "    print (\" \",llave, \"  N:\",len(list(df_select.reference_UT.unique())), \"  avg # ref:\",df_select.drop_duplicates(subset=['paper_UT']).total_refs.mean())\n",
    "    max_key_plos=llave\n",
    "\n",
    "\n",
    "    if cont ==0:\n",
    "        min_key_plos = llave\n",
    "    cont  +=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similartly, I get the data in the percentiles for references' citations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############## i define quantiles for references (\"TOP\" and \"BOTTOM\") \n",
    "list_q_ref=[.1,.9,1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_for_quantiles_ref = preselection_df3.drop_duplicates(subset=['reference_UT'])   # ojo!!! remember to remove REPETITIONS!!!!\n",
    "quantiles=sorted(list(df_for_quantiles_ref['cite_count'].quantile(list_q_ref).to_dict().items())) #mean 10.68 \n",
    "print (\"\\n\\ncitation bins for the references in the selected plos:\", list_q_ref,quantiles)    \n",
    "\n",
    "lista_bins=[]\n",
    "old_value=0\n",
    "for item in quantiles:\n",
    "    pair=[old_value, int(item[1])]\n",
    "    lista_bins.append(pair)\n",
    "    old_value = int(item[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print (\"\\nbins for refrences:\")\n",
    "\n",
    "\n",
    "cont = 0\n",
    "dict_bin_list_ref_UT={}\n",
    "for item in lista_bins:\n",
    "\n",
    "    minimo = item[0]\n",
    "    maximo = item[1]    \n",
    "\n",
    "    df_select = preselection_df3[(preselection_df3['cite_count'] >= minimo)  &  (preselection_df3['cite_count'] < maximo)]\n",
    "    llave=str(minimo)+\"-\"+str(maximo)\n",
    "    dict_bin_list_ref_UT[llave]=list(df_select.reference_UT.unique())\n",
    "    print (\" \",llave, \"N:\",len(list(df_select.reference_UT.unique())), \"  avg # ref:\",df_select.drop_duplicates(subset=['reference_UT']).total_refs.mean())\n",
    "    max_key_ref=llave\n",
    "\n",
    "    if cont ==0:\n",
    "        min_key_ref = llave\n",
    "    cont  +=1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### I create the list of IDs of top papers, top references, bottom papers and bottom references that I will be comparing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lista_top_plos = dict_bin_list_plos_UT[max_key_plos]\n",
    "print (\"\\n\\n# UTs top\",(100-100*list_q_plos[-2]),\"% plos:\",len(lista_top_plos))\n",
    "\n",
    "lista_top_ref=dict_bin_list_ref_UT[max_key_ref]\n",
    "print (\"# UTs top\",(100-100*list_q_ref[-2]),\"% ref:\", len(lista_top_ref))\n",
    "\n",
    "\n",
    "lista_bottom_plos = dict_bin_list_plos_UT[min_key_plos]\n",
    "print (\"# UTs bottom \",(100*list_q_plos[0]),\"% plos:\",len(lista_bottom_plos))\n",
    "\n",
    "lista_bottom_ref=dict_bin_list_ref_UT[min_key_ref]\n",
    "print (\"# UTs bottom \",(100*list_q_ref[0]),\"% ref:\", len(lista_bottom_ref))\n",
    "\n",
    "list_plos_in_year= list(preselection_df3.paper_UT.unique())\n",
    "print (\"Tot # records:\",len(preselection_df3),\", # plos:\",len(list_plos_in_year))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  I obtain at the usage of the top references  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_top_ref = preselection_df3[preselection_df3['reference_UT'].isin(lista_top_ref)]\n",
    "\n",
    "\n",
    "df_top_ref_top_plos = df_top_ref[df_top_ref['paper_UT'].isin(lista_top_plos)]\n",
    "df_top_ref_bottom_plos = df_top_ref[df_top_ref['paper_UT'].isin(lista_bottom_plos)]\n",
    "\n",
    "\n",
    "usage_top_ref_top_plos = len(df_top_ref_top_plos)/float(len(df_top_ref))\n",
    "usage_top_ref_bottom_plos = len(df_top_ref_bottom_plos)/float(len(df_top_ref))\n",
    "\n",
    "\n",
    "print (\"fraction of usage of top ref by \")\n",
    "print (\"  top\",(100-100*list_q_plos[-2]),\"% plos:\",  usage_top_ref_top_plos)\n",
    "print (\"  bottom\",(100*list_q_plos[0]),\"% plos:\",usage_top_ref_bottom_plos  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I obtain at the usage of the bottom references  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_top_ref = preselection_df3[preselection_df3['reference_UT'].isin(lista_bottom_ref)]\n",
    "\n",
    "\n",
    "df_non_top_ref_top_plos = df_non_top_ref[df_non_top_ref['paper_UT'].isin(lista_top_plos)]\n",
    "df_non_top_ref_bottom_plos = df_non_top_ref[df_non_top_ref['paper_UT'].isin(lista_bottom_plos)]\n",
    "\n",
    "usage_non_top_ref_top_plos = len(df_non_top_ref_top_plos)/float(len(df_non_top_ref))\n",
    "usage_non_top_ref_bottom_plos = len(df_non_top_ref_bottom_plos)/float(len(df_non_top_ref))\n",
    "\n",
    "\n",
    "print (\"fraction of usage of non-top ref by \")\n",
    "print (\"  top\",(100-100*list_q_plos[-2]),\"% plos:\", usage_non_top_ref_top_plos )\n",
    "print (\"  bottom\",(100*list_q_plos[0]),\"% plos:\", usage_non_top_ref_bottom_plos )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now I have the actual usage. \n",
    "#### Next, I enter the loop to canculate the expected values from my null model (usage of references by top and non top plos papers, from the randomized data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lista_usage_top_ref_by_top_plos_rand = []\n",
    "lista_usage_top_ref_by_bottom_plos_rand = []\n",
    "\n",
    "lista_usage_nontop_ref_by_top_plos_rand = []\n",
    "lista_usage_nontop_ref_by_bottom_plos_rand = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### first i get the list of lists corresponding to the references used in the selected df, preserving the reference grouping or isolation:  \n",
    "lista_lists_values = get_list_lists_references(preselection_df3)\n",
    "\n",
    "print (\"len list_lists_all_ref:\",len(lista_lists_values), preselection_df3.shape)\n",
    "\n",
    "\n",
    "for i in range(Niter):\n",
    "\n",
    "    print (i)\n",
    "\n",
    "\n",
    "    ########   new randomization scheme (controling for year, but also preserving groups of references cited together in a paper):   \n",
    "    #### lista_values is created outside the Niter loop   (i only need to do it once per selected df)    \n",
    "\n",
    "\n",
    "\n",
    "    random.shuffle(lista_lists_values)      \n",
    "    ### to flat out a list of lists:   \n",
    "    flat_list = []    \n",
    "    for sublist in lista_lists_values:\n",
    "        for item in sublist:\n",
    "            flat_list.append(item)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    preselection_df3['randomized_ref_UT'] = flat_list   ### (this randomizes paper_UT, not reference_UT)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ####### (RANDOMIZED)  i look at the usage of the top ref\n",
    "    df_top_ref_rand = preselection_df3[preselection_df3['randomized_ref_UT'].isin(lista_top_ref)]\n",
    "\n",
    "    df_top_ref_top_plos_rand = df_top_ref_rand[df_top_ref_rand['paper_UT'].isin(lista_top_plos)]\n",
    "    df_top_ref_bottom_plos_rand = df_top_ref_rand[df_top_ref_rand['paper_UT'].isin(lista_bottom_plos)]\n",
    "\n",
    "\n",
    "    usage_top_ref_top_plos_rand = len(df_top_ref_top_plos_rand)/float(len(df_top_ref_rand))\n",
    "    usage_top_ref_bottom_plos_rand = len(df_top_ref_bottom_plos_rand)/float(len(df_top_ref_rand))\n",
    "\n",
    "\n",
    "    lista_usage_top_ref_by_top_plos_rand.append(usage_top_ref_top_plos_rand)\n",
    "    lista_usage_top_ref_by_bottom_plos_rand.append(usage_top_ref_bottom_plos_rand)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #######  (RANDOMIZED) i look at the usage of the non-top ref            \n",
    "    df_non_top_ref_rand = preselection_df3[preselection_df3['randomized_ref_UT'].isin(lista_bottom_ref)]        \n",
    "\n",
    "    df_non_top_ref_top_plos_rand = df_non_top_ref_rand[df_non_top_ref_rand['paper_UT'].isin(lista_top_plos)]\n",
    "    df_non_top_ref_bottom_plos_rand = df_non_top_ref_rand[df_non_top_ref_rand['paper_UT'].isin(lista_bottom_plos)]\n",
    "\n",
    "\n",
    "    usage_non_top_ref_top_plos_rand = len(df_non_top_ref_top_plos_rand)/float(len(df_non_top_ref_rand))\n",
    "    usage_non_top_ref_bottom_plos_rand = len(df_non_top_ref_bottom_plos_rand)/float(len(df_non_top_ref_rand))\n",
    "\n",
    "\n",
    "    lista_usage_nontop_ref_by_top_plos_rand.append(usage_non_top_ref_top_plos_rand)\n",
    "    lista_usage_nontop_ref_by_bottom_plos_rand.append(usage_non_top_ref_bottom_plos_rand)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print (\"\\n\\n\\n\\navg randomized!!\")\n",
    "print(\"fraction of usage of top ref by\")\n",
    "print (\"  top\",(100-100*list_q_plos[-2]),\"% plos:\",  np.mean(lista_usage_top_ref_by_top_plos_rand) )   \n",
    "print (\"  bottom\",(100*list_q_plos[0]),\"% plos:\",np.mean(lista_usage_top_ref_by_bottom_plos_rand)  )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print (\"\\n\\navg randomized\")\n",
    "print (\"fraction of usage of non-top ref by \")\n",
    "print (\"  top\",(100-100*list_q_plos[-2]),\"% plos:\", np.mean(lista_usage_nontop_ref_by_top_plos_rand) )   \n",
    "print (\"  bottom\",(100*list_q_plos[0]),\"% plos:\", np.mean(lista_usage_nontop_ref_by_bottom_plos_rand) ,\"\\n\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  I get the corresponding z-scores from the comparisons actual vs. expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lista_bin_names = ['Bottom '+str(int(list_q_plos[0]*100))+'%<br>papers', 'Top '+str(int(100-100*list_q_plos[-2]))+'%<br>papers']\n",
    "\n",
    "lista_for_top_ref = [ usage_top_ref_top_plos, usage_top_ref_bottom_plos]\n",
    "lista_for_bottom_ref = [usage_non_top_ref_top_plos, usage_non_top_ref_bottom_plos]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lista_for_top_ref = [  usage_top_ref_bottom_plos, usage_top_ref_top_plos]\n",
    "lista_for_bottom_ref = [usage_non_top_ref_bottom_plos, usage_non_top_ref_top_plos]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### this is the null model \n",
    "lista_expectations_top_ref = [np.mean(lista_usage_top_ref_by_bottom_plos_rand), np.mean(lista_usage_top_ref_by_top_plos_rand)]  \n",
    "lista_expectations_bottom_ref = [ np.mean(lista_usage_nontop_ref_by_bottom_plos_rand),np.mean(lista_usage_nontop_ref_by_top_plos_rand)] \n",
    "\n",
    "\n",
    "list_errors_top_ref = [2.*np.std(lista_usage_top_ref_by_bottom_plos_rand), 2.*np.std(lista_usage_top_ref_by_top_plos_rand) ] \n",
    "list_errors_bottom_ref = [2.*np.std(lista_usage_nontop_ref_by_bottom_plos_rand), 2.*np.std(lista_usage_nontop_ref_by_top_plos_rand) ] \n",
    "\n",
    "\n",
    "z_score_top_ref_by_top_plos = (usage_top_ref_top_plos - np.mean(lista_usage_top_ref_by_top_plos_rand))/np.std(lista_usage_top_ref_by_top_plos_rand)\n",
    "z_score_nontop_ref_by_top_plos = (usage_non_top_ref_top_plos - np.mean(lista_usage_nontop_ref_by_top_plos_rand))/np.std(lista_usage_nontop_ref_by_top_plos_rand)\n",
    "\n",
    "z_score_top_ref_by_bottom_plos = (usage_top_ref_bottom_plos - np.mean(lista_usage_top_ref_by_bottom_plos_rand))/np.std(lista_usage_top_ref_by_bottom_plos_rand)\n",
    "z_score_nontop_ref_by_bottom_plos = (usage_non_top_ref_bottom_plos - np.mean(lista_usage_nontop_ref_by_bottom_plos_rand))/np.std(lista_usage_nontop_ref_by_bottom_plos_rand)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print ('zscore top ref cited  by top plos:', z_score_top_ref_by_top_plos)\n",
    "print ('zscore bottom ref cited  by top plos:', z_score_nontop_ref_by_top_plos)\n",
    "\n",
    "print ('zscore top ref cited  by bottom plos:', z_score_top_ref_by_bottom_plos)\n",
    "print ('zscore bottom ref cited  by bottom plos:', z_score_nontop_ref_by_bottom_plos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIGURE: plot the results from the randomization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "title_string=''\n",
    "\n",
    "\n",
    "\n",
    "trace1 = go.Bar(\n",
    "    x=lista_bin_names,\n",
    "    y=lista_for_top_ref,    \n",
    "    marker=dict(\n",
    "            color='#88419d',           \n",
    "    ),\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "trace2 = go.Bar(\n",
    "    x=lista_bin_names,\n",
    "    y=lista_expectations_top_ref,\n",
    "    error_y=dict(           \n",
    "            array=list_errors_top_ref,\n",
    "            thickness=5,\n",
    "            visible=True\n",
    "    ),\n",
    "    marker=dict(\n",
    "            color='#c994c7',         \n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "trace3 = go.Bar(\n",
    "    x=lista_bin_names,\n",
    "    y=lista_for_bottom_ref,\n",
    "      marker=dict(\n",
    "            color='#225ea8',   \n",
    "            ),\n",
    ")\n",
    "\n",
    "\n",
    "trace4 = go.Bar(\n",
    "    x=lista_bin_names,\n",
    "    y=lista_expectations_bottom_ref,\n",
    "    error_y=dict(       \n",
    "            array=list_errors_bottom_ref,#[0.5, 1, 2],\n",
    "            thickness=5,\n",
    "            visible=True\n",
    "            ),\n",
    "    marker=dict(\n",
    "            color='#a6bddb',     \n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "data = [trace1, trace2, trace3, trace4]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Layout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "size_bar_name = 30\n",
    "y_pos_bar_names = -.039\n",
    "angle = -70\n",
    "font_gral=30\n",
    "\n",
    "layout = go.Layout(   \n",
    "    title=title_string,\n",
    "    xaxis = dict(\n",
    "        side= 'top',\n",
    "        range = [-.5,1.5],\n",
    "       # showline =  True,\n",
    "        #title= 'Plos Citation percentile'),\n",
    "    ),\n",
    "    yaxis = dict(\n",
    "        title= 'Fraction of references cited',\n",
    "        range = [-.07,0.17],\n",
    "        tickvals=[0.0,0.05,0.1,0.15],\n",
    "        #showline =  True,\n",
    "         ),\n",
    "\n",
    "    showlegend=False,\n",
    "    bargroupgap=0.15,\n",
    "\n",
    "\n",
    "    #### ADD ANNOTATIONS:\n",
    "    annotations = [  \n",
    "        # the four bars on the left\n",
    "        dict(\n",
    "          x = -.34,\n",
    "          y = y_pos_bar_names,\n",
    "          text = 'Top '+str(int(100-100*list_q_ref[-2]))+'%<br>references',\n",
    "          textangle=angle,\n",
    "            font = dict(size = size_bar_name ),\n",
    "           ),    \n",
    "        dict(\n",
    "          x = -0.14,\n",
    "          y = y_pos_bar_names,\n",
    "          text = 'Null Model',\n",
    "           textangle=angle,\n",
    "            font = dict(size = size_bar_name ),\n",
    "           ),\n",
    "        dict(\n",
    "          x = .08,\n",
    "          y = y_pos_bar_names,\n",
    "          showarrow = False,\n",
    "          text = 'Bottom '+str(int(list_q_ref[0]*100))+'%<br>references', \n",
    "          textangle=angle,\n",
    "            font = dict(size = size_bar_name ),\n",
    "           ),\n",
    "        dict(\n",
    "          x = .28,\n",
    "          y = y_pos_bar_names,\n",
    "          text =  'Null Model',\n",
    "          textangle=angle,\n",
    "            font = dict(size = size_bar_name ),\n",
    "           ),\n",
    "\n",
    "\n",
    "\n",
    "        # the four bars on the right\n",
    "        dict(  \n",
    "          x = .68,\n",
    "          y = y_pos_bar_names,\n",
    "          text = 'Top '+str(int(100-100*list_q_ref[-2]))+'%<br>references',\n",
    "          textangle=angle,\n",
    "            font = dict(size = size_bar_name ),\n",
    "           ),\n",
    "        dict(\n",
    "          x = .88,\n",
    "          y = y_pos_bar_names,\n",
    "          text = 'Null Model',\n",
    "           textangle=angle,\n",
    "            font = dict(size = size_bar_name ),\n",
    "           ),\n",
    "        dict(\n",
    "          x = 1.08,\n",
    "          y = y_pos_bar_names,\n",
    "          text = 'Bottom '+str(int(list_q_ref[0]*100))+'%<br>references', \n",
    "          textangle=angle,\n",
    "            font = dict(size = size_bar_name ),\n",
    "           ),\n",
    "        dict(\n",
    "          x = 1.28,\n",
    "          y = y_pos_bar_names,\n",
    "          text =  'Null Model',\n",
    "          textangle=angle,\n",
    "            font = dict(size = size_bar_name ),\n",
    "           ),\n",
    "\n",
    "        ],    \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig['layout']['font']['size'] = font_gral-5   \n",
    "fig['layout']['xaxis']['tickangle'] = 0\n",
    "fig['layout']['xaxis']['tickfont']['size'] = font_gral -5\n",
    "fig['layout']['yaxis']['tickfont']['size'] = font_gral -10\n",
    "fig['layout']['margin']=dict(\n",
    "        l=200,\n",
    "       # r=50,\n",
    "        b=100,\n",
    "        t=200,\n",
    "        pad=15\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "fig_filename='null_model_fract_usage_top_bottom_ref_'+str(years[0])+string_code_categ.replace(\" \",\"_\")+\"_\"+str(Niter)+\"iter\"\n",
    "offline.plot(fig, auto_open=True, image = 'png', image_filename='../plots/'+fig_filename ,image_width=2000, image_height=1200, filename=fig_filename+'.html', validate=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This would be the result:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/results_from_randomization_actual_usage_vs_expected.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIGURE CAPTION (THIS RESULTS ARE UNPUBLISHED YET, PLEASE KEEP IT CONFIDENTIAL): The usage of references by top and bottom papers is significantly different than what could be expected at random. After controlling for publication year, field and group vs isolated references, we find that top papers used a higher-than-expected fraction of top references, and a lower-than-expected fraction of bottom references. The exact opposite is true for bottom papers: they use a lower-than-expected fraction of top references, and a higher-than-expected fraction of bottom references. Definition of top and bottom papers (or references) is given by the 10% top or bottom percentiles of number of citations accrued by said papers (or references)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
